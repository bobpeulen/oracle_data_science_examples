{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "468deebf",
   "metadata": {},
   "source": [
    "# **DeepFace - Passport vs Profile Image - Invoke and run from APEX**\n",
    "\n",
    "- Runs deep face pipeline (detecting faces, cropping image, flatten image, extract face, etc)\n",
    "- Cross-checks detected face(s) from two images\n",
    "- Extracts predicted agen and predicted gender from both"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0050655",
   "metadata": {},
   "source": [
    "## **1. Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574c3937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #1.st define bucket and namespace where to store\n",
    "# !odsc conda init -b conda_environment_yolov5 -n frqap2zhtzbe -a resource_principal\n",
    "\n",
    "# #2.nd publish conda env to bucket\n",
    "# !odsc conda publish -s tensorflow28_p38_gpu_v1 --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4204e3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface import DeepFace\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import fsspec\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from ads.model.framework.tensorflow_model import TensorFlowModel\n",
    "from ads.common.model_metadata import UseCaseType\n",
    "from ads.common.model_artifact import ModelArtifact\n",
    "from ads.common.model_export_util import prepare_generic_model\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, Convolution2D\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import keras.models\n",
    "import tensorflow\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "from keras.models import model_from_json, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec09a883",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dde15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install opencv-python-headless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf8b3bb",
   "metadata": {},
   "source": [
    "## **2. Test Deepface locally**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f2377a",
   "metadata": {},
   "source": [
    "### This will download the first time the model weights locally in \" /home/datascience /.deepface/weights\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d242dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Can not use Deepface like the below in model deployment. It will try and download the model and store locally. This will fail.\n",
    "##\n",
    "\n",
    "from deepface import DeepFace \n",
    "passport_input = \"./example_images/bob_pass.jpg\"            #my passport\n",
    "profile_image_input = \"./example_images/bob_pf_1.jpg\"       #profile image\n",
    "\n",
    "result = DeepFace.verify(passport_input, profile_image_input)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f6bc34",
   "metadata": {},
   "source": [
    "## **3. Create model artifacts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5848c9c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop1:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:ADS:As force_overwrite is set to True, all the existing files in the /home/datascience/model_artifacts will be removed\n",
      "WARNING:ads.common:Generating runtime.yaml template. This file needs to be updated before saving it to the model catalog.\n",
      "The inference conda environment is tensorflow28_p38_gpu_v1 and the Python version is 3.7.\n",
      "WARNING:ADS:Taxonomy metadata was not extracted. To auto-populate taxonomy metadata the model must be provided. Pass the model as a parameter to .prepare_generic_model(model=model, usecase_type=UseCaseType.REGRESSION). Alternative way is using atifact.populate_metadata(model=model, usecase_type=UseCaseType.REGRESSION).\n"
     ]
    }
   ],
   "source": [
    "#path to artifacts and conda slug\n",
    "path_to_artifacts = '/home/datascience/model_artifacts'\n",
    "\n",
    "#create default artifacts\n",
    "artifact = prepare_generic_model(path_to_artifacts, fn_artifact_files_included=False, force_overwrite=True, inference_conda_env=\"tensorflow28_p38_gpu_v1\", inference_python_version=\"3.7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbc2983d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/datascience/model_artifacts/runtime.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"{path_to_artifacts}/runtime.yaml\"\n",
    "\n",
    "# Model runtime environment\n",
    "MODEL_ARTIFACT_VERSION: '3.0'\n",
    "MODEL_DEPLOYMENT:\n",
    "  INFERENCE_CONDA_ENV:\n",
    "    INFERENCE_ENV_PATH: oci://conda_environment_yolov5@frqap2zhtzbe/conda_environments/gpu/TensorFlow 2.8 for GPU on Python 3.8/1.0/tensorflow28_p38_gpu_v1\n",
    "    INFERENCE_ENV_SLUG: tensorflow28_p38_gpu_v1\n",
    "    INFERENCE_ENV_TYPE: published\n",
    "    INFERENCE_PYTHON_VERSION: '3.8'\n",
    "MODEL_PROVENANCE:\n",
    "  PROJECT_OCID: ocid1.datascienceproject.oc1.eu-frankfurt-1.amaaaaaangencdyaik5ssdqk4as2bhldxprh7vnqpk7yycsm7vymd344cgua\n",
    "  TENANCY_OCID: ocid1.tenancy.oc1..aaaaaaaabu5fgingcjq3vc7djuwsdcutdxs4gsws6h4kfoldqpjuggxprgoa\n",
    "  TRAINING_COMPARTMENT_OCID: ocid1.compartment.oc1..aaaaaaaae3n6r6hrjipbap2hojicrsvkzatrtlwvsyrpyjd7wjnw4za3m75q\n",
    "  TRAINING_CONDA_ENV:\n",
    "    TRAINING_ENV_PATH: oci://conda_environment_yolov5@frqap2zhtzbe/conda_environments/gpu/TensorFlow 2.8 for GPU on Python 3.8/1.0/tensorflow28_p38_gpu_v1\n",
    "    TRAINING_ENV_SLUG: tensorflow28_p38_gpu_v1\n",
    "    TRAINING_ENV_TYPE: published\n",
    "    TRAINING_PYTHON_VERSION: '3.8'\n",
    "  TRAINING_REGION: eu-frankfurt-1\n",
    "  TRAINING_RESOURCE_OCID: ocid1.datasciencenotebooksession.oc1.eu-frankfurt-1.amaaaaaangencdyacxmsz5ycch762wjc54udhibtl3m4nacuaf7shrvyoktq\n",
    "  USER_OCID: ocid1.saml2idp.oc1..aaaaaaaar3ydw5hoiob7dfjzoom2dvbhqkkd5fat6m7upe72emlsxhsfrbfa/bob.peulen@oracle.com\n",
    "  VM_IMAGE_INTERNAL_ID: NB1480-DCGPU131-VMP64-VMA1585-BI681"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379a72eb",
   "metadata": {},
   "source": [
    "## **4. Get model weights and test locally**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b9eaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the weights to the model artifact folder\n",
    "!cp /home/datascience/.deepface/weights/vgg_face_weights.h5 /home/datascience/model_artifacts\n",
    "!cp /home/datascience/.deepface/weights/gender_model_weights.h5 /home/datascience/model_artifacts\n",
    "!cp /home/datascience/.deepface/weights/age_model_weights.h5 /home/datascience/model_artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3c4018",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### functions\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=(224, 224))\n",
    "    \n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "def findCosineSimilarity(source_representation, test_representation):\n",
    "    a = np.matmul(np.transpose(source_representation), test_representation)\n",
    "    b = np.sum(np.multiply(source_representation, source_representation))\n",
    "    c = np.sum(np.multiply(test_representation, test_representation))\n",
    "    return 1 - (a / (np.sqrt(b) * np.sqrt(c)))\n",
    "\n",
    "def findEuclideanDistance(source_representation, test_representation):\n",
    "    euclidean_distance = source_representation - test_representation\n",
    "    euclidean_distance = np.sum(np.multiply(euclidean_distance, euclidean_distance))\n",
    "    euclidean_distance = np.sqrt(euclidean_distance)\n",
    "    return euclidean_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d90e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "### define model\n",
    "model = Sequential()\n",
    "model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(2622, (1, 1)))\n",
    "model.add(Flatten())\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f0d03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load weights in model\n",
    "model.load_weights('/home/datascience/model_artifacts/vgg_face_weights.h5')\n",
    "\n",
    "#define input and ouput layer\n",
    "vgg_face_descriptor = Model(inputs=model.layers[0].input, outputs=model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdc709c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #save model locally in model artifacts\n",
    "# vgg_face_descriptor.save(\"/home/datascience/model_artifacts/model_artifacts\")\n",
    "\n",
    "# model = keras.models.load_model(\"/home/datascience/model_artifacts/model_artifacts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f16552",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test on two images\n",
    "passport_input = \"./example_images/bob_pass.jpg\"\n",
    "profile_image_input = \"./example_images/bob_pf_1.jpg\"\n",
    "\n",
    "#define distance and sensitivity\n",
    "epsilon = 0.60 #cosine similarity\n",
    "#epsilon = 120 #euclidean distance\n",
    " \n",
    "img1_representation = vgg_face_descriptor.predict(preprocess_image(passport_input))[0,:]\n",
    "img2_representation = vgg_face_descriptor.predict(preprocess_image(profile_image_input))[0,:]\n",
    " \n",
    "cosine_similarity = findCosineSimilarity(img1_representation, img2_representation)\n",
    "#euclidean_distance = findEuclideanDistance(img1_representation, img2_representation)\n",
    " \n",
    "if(cosine_similarity < epsilon):\n",
    "    print(\"Same person\")\n",
    "else:\n",
    "    print(\"Not same person!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe2bb4d",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01afabc9",
   "metadata": {},
   "source": [
    "## **5. Score.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "724aa456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/datascience/model_artifacts/score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"{path_to_artifacts}/score.py\"\n",
    "\n",
    "########################################\n",
    "########################################\n",
    "########################################\n",
    "########################################\n",
    "########################################\n",
    "#################### Imports\n",
    "#################### \n",
    "####################\n",
    "####################\n",
    "\n",
    "import glob      \n",
    "import os\n",
    "import base64\n",
    "import uuid\n",
    "import io\n",
    "from deepface import DeepFace\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import fsspec\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from ads.model.framework.tensorflow_model import TensorFlowModel\n",
    "from ads.common.model_metadata import UseCaseType\n",
    "from ads.common.model_artifact import ModelArtifact\n",
    "from ads.common.model_export_util import prepare_generic_model\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, Convolution2D\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import keras.models\n",
    "import tensorflow\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "from keras.models import model_from_json, Model\n",
    "import shutil\n",
    "\n",
    "########################################\n",
    "########################################\n",
    "########################################\n",
    "########################################\n",
    "########################################\n",
    "#################### Model\n",
    "#################### \n",
    "####################\n",
    "####################\n",
    "\n",
    "def load_model():                                        ### check to load_model instead of in predict\n",
    "    \n",
    "#     #load weights in model\n",
    "#     #model.load_weights('./.deepface/weights/vgg_face_weights.h5')   #in model deployment\n",
    "#     model.load_weights('./model_artifacts/.deepface/weights/vgg_face_weights.h5')   #locally notebook\n",
    "\n",
    "#     #define input and ouput layer\n",
    "#     vgg_face_descriptor = Model(inputs=model.layers[0].input, outputs=model.layers[-2].output)\n",
    "    \n",
    "#     return vgg_face_descriptor\n",
    "\n",
    "\n",
    "    class DummyModel:\n",
    "        def __init__(self):\n",
    "            pass\n",
    "    return DummyModel()\n",
    "\n",
    "########################################\n",
    "########################################\n",
    "########################################\n",
    "########################################\n",
    "########################################\n",
    "#################### Predict\n",
    "#################### \n",
    "####################\n",
    "####################\n",
    "\n",
    "def predict(data, model=load_model()):\n",
    "    \n",
    "    #get the base64 images from the payload\n",
    "    passport_data = data['data']['passport_data']\n",
    "    profile_data = data['data']['profile_data']\n",
    "    \n",
    "    #input image folder\n",
    "    path_input_image_locally = \"/home/datascience/images\" \n",
    "    \n",
    "    #delete folder when exists\n",
    "    if os.path.exists(path_input_image_locally):\n",
    "        shutil.rmtree(path_input_image_locally)\n",
    "    \n",
    "    #make as new folder\n",
    "    if not os.path.exists(path_input_image_locally):         \n",
    "        os.makedirs(path_input_image_locally)\n",
    "        \n",
    "       \n",
    "    ##### decoding of passport\n",
    "    img_bytes_p = io.BytesIO(base64.b64decode(passport_data.encode('utf-8')))\n",
    "    passport_image = Image.open(img_bytes_p).resize((224, 224))  #same as input layer? and pre processing\n",
    "    \n",
    "    #save image locally     \n",
    "    passport_image = passport_image.save(path_input_image_locally + \"/passport_image.jpg\")\n",
    "    \n",
    "    ##### decoding of profile\n",
    "    img_bytes_d = io.BytesIO(base64.b64decode(profile_data.encode('utf-8')))\n",
    "    profile_image = Image.open(img_bytes_d).resize((224, 224))  #same as input layer? and pre processing\n",
    "    \n",
    "    #save image locally     \n",
    "    profile_image = profile_image.save(path_input_image_locally + \"/profile_image.jpg\")\n",
    "    \n",
    "    #make prediction. Deepface tries to load from externally. But changed the path below. Verify is a pipeline of steps. Also extracts face from image, 3d to 2d, etc.\n",
    "    result_deepface = DeepFace.verify(path_input_image_locally + \"/passport_image.jpg\", img2_path = path_input_image_locally + \"/profile_image.jpg\")\n",
    "    \n",
    "    #extract key stats\n",
    "    verified = result_deepface['verified']   # need to convert this to text. json doesn't like True or False\n",
    "    distance = result_deepface['distance']\n",
    "    \n",
    "    verified_text = str()\n",
    "    #turn bool (false true) into text\n",
    "    if verified == True:\n",
    "        verified_text = \"Same person\"\n",
    "    else:\n",
    "        verified_text = \"Different persons\"\n",
    "        \n",
    "#     ######\n",
    "#     ###### output faces\n",
    "#     detected_face_profile = DeepFace.detectFace(\"./profile_image.jpg\", detector_backend = 'opencv')\n",
    "#     detected_face = detected_face * 255\n",
    "#     cv2.imwrite(\"face.jpg\", detected_face[:, :, ::-1])\n",
    "\n",
    "    #### \n",
    "    #### age and gender\n",
    "    objs_profile = DeepFace.analyze(img_path = path_input_image_locally + \"/profile_image.jpg\", actions = ['age', 'gender'])\n",
    "    objs_passport = DeepFace.analyze(img_path = path_input_image_locally + \"/passport_image.jpg\", actions = ['age', 'gender'])\n",
    "    \n",
    "    age_prof = objs_profile[0]['age']\n",
    "    gender_prof = objs_profile[0]['dominant_gender']\n",
    "    age_pass = objs_passport[0]['age']\n",
    "    gender_pass = objs_passport[0]['dominant_gender']\n",
    "\n",
    "    return {'prediction_deepface': {'verified': verified_text, 'distance': distance, 'age_prof': age_prof, 'gender_prof':gender_prof, 'age_pass':age_pass, 'gender_pass': gender_pass}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c75e14",
   "metadata": {},
   "source": [
    "## **Create a payload. Two images in one base64 string**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91baf1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/datascience/1_projects/deepface/testx_images/bob_pass.jpg\n",
      "/home/datascience/1_projects/deepface/testx_images/bob_pf_1.jpg\n"
     ]
    }
   ],
   "source": [
    "## first passport\n",
    "## second profile\n",
    "\n",
    "import glob      \n",
    "import os\n",
    "import base64\n",
    "import uuid\n",
    "import io\n",
    "\n",
    "file_path=\"/home/datascience/1_projects/deepface/testx_images/\"\n",
    "\n",
    "payload_string = str()\n",
    "full_payload_string = str()\n",
    "counter = 0\n",
    "\n",
    "for images in os.listdir(file_path):\n",
    "    \n",
    "    counter += 1\n",
    "    \n",
    "    input_path=os.path.join(file_path, images)\n",
    "    print(input_path)\n",
    "    \n",
    "    with open(input_path, \"rb\") as image2string:\n",
    "        converted_string = base64.b64encode(image2string.read()).decode('ascii')\n",
    "               \n",
    "        #add payload to full string\n",
    "        \n",
    "        if counter == 1:\n",
    "            payload1 = json.dumps(converted_string)\n",
    "            json_payload1 = json.loads(payload1)\n",
    "            \n",
    "        else: #if counter is 2\n",
    "            payload2 = json.dumps(converted_string)\n",
    "            json_payload2 = json.loads(payload2)\n",
    "\n",
    "payload_json = {'data':{'passport_data': json_payload1,'profile_data': json_payload2}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b700b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -r /home/datascience/images_test/.ipynb_checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1689b60e",
   "metadata": {},
   "source": [
    "## **Model Catalog**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e49b4976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'score.py', 'runtime.yaml', 'test_json_output.json', 'gender_model_weights.h5', 'age_model_weights.h5', 'vgg_face_weights.h5']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test key</th>\n",
       "      <th>Test name</th>\n",
       "      <th>Result</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>runtime_env_path</td>\n",
       "      <td>Check that field MODEL_DEPLOYMENT.INFERENCE_ENV_PATH is set</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>runtime_env_python</td>\n",
       "      <td>Check that field MODEL_DEPLOYMENT.INFERENCE_PYTHON_VERSION is set to a value of 3.6 or higher</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>runtime_path_exist</td>\n",
       "      <td>Check that the file path in MODEL_DEPLOYMENT.INFERENCE_ENV_PATH is correct.</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>runtime_version</td>\n",
       "      <td>Check that field MODEL_ARTIFACT_VERSION is set to 3.0</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>runtime_yaml</td>\n",
       "      <td>Check that the file \"runtime.yaml\" exists and is in the top level directory of the artifact directory</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>score_load_model</td>\n",
       "      <td>Check that load_model() is defined</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>score_predict</td>\n",
       "      <td>Check that predict() is defined</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>score_predict_arg</td>\n",
       "      <td>Check that all other arguments in predict() are optional and have default values</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>score_predict_data</td>\n",
       "      <td>Check that the only required argument for predict() is named \"data\"</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>score_py</td>\n",
       "      <td>Check that the file \"score.py\" exists and is in the top level directory of the artifact directory</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>score_syntax</td>\n",
       "      <td>Check for Python syntax errors</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Test key  \\\n",
       "0     runtime_env_path   \n",
       "1   runtime_env_python   \n",
       "2   runtime_path_exist   \n",
       "3      runtime_version   \n",
       "4         runtime_yaml   \n",
       "5     score_load_model   \n",
       "6        score_predict   \n",
       "7    score_predict_arg   \n",
       "8   score_predict_data   \n",
       "9             score_py   \n",
       "10        score_syntax   \n",
       "\n",
       "                                                                                                Test name  \\\n",
       "0                                             Check that field MODEL_DEPLOYMENT.INFERENCE_ENV_PATH is set   \n",
       "1           Check that field MODEL_DEPLOYMENT.INFERENCE_PYTHON_VERSION is set to a value of 3.6 or higher   \n",
       "2                             Check that the file path in MODEL_DEPLOYMENT.INFERENCE_ENV_PATH is correct.   \n",
       "3                                                   Check that field MODEL_ARTIFACT_VERSION is set to 3.0   \n",
       "4   Check that the file \"runtime.yaml\" exists and is in the top level directory of the artifact directory   \n",
       "5                                                                      Check that load_model() is defined   \n",
       "6                                                                         Check that predict() is defined   \n",
       "7                        Check that all other arguments in predict() are optional and have default values   \n",
       "8                                     Check that the only required argument for predict() is named \"data\"   \n",
       "9       Check that the file \"score.py\" exists and is in the top level directory of the artifact directory   \n",
       "10                                                                         Check for Python syntax errors   \n",
       "\n",
       "    Result Message  \n",
       "0   Passed          \n",
       "1   Passed          \n",
       "2   Passed          \n",
       "3   Passed          \n",
       "4   Passed          \n",
       "5   Passed          \n",
       "6   Passed          \n",
       "7   Passed          \n",
       "8   Passed          \n",
       "9   Passed          \n",
       "10  Passed          "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all should be passed\n",
    "artifact.introspect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13a219da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop1:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'ocid1.datasciencemodel.oc1.eu-frankfurt-1.amaaaaaangencdyagoq64zzokto5oxxl65m32yjueb2yggu36uy4pq57rj5a'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the model artifact to the model catalog. \n",
    "catalog_entryx = artifact.save(display_name='deepface_oda_v8', description='deepface_oda_v8_changed_payload', timeout=600)\n",
    "\n",
    "catalog_entryx.id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cd884f",
   "metadata": {},
   "source": [
    "## **Model Deployment invoke**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e09f416e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "{'prediction_deepface': {'verified': 'Same person', 'distance': 0.3519110140477171, 'age_prof': 27, 'gender_prof': 'Man', 'age_pass': 24, 'gender_pass': 'Man'}}\n",
      "CPU times: user 53 ms, sys: 2.39 ms, total: 55.4 ms\n",
      "Wall time: 3.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import requests\n",
    "import oci\n",
    "from oci.signer import Signer\n",
    "import json\n",
    "\n",
    "# REST API\n",
    "uri = f\"https://modeldeployment.eu-frankfurt-1.oci.customer-oci.com/ocid1.datasciencemodeldeployment.oc1.eu-frankfurt-1.amaaaaaangencdya3p3wmufctj23w3t4o25xla77eryywml7q2oti6sai7cq/predict\"\n",
    "\n",
    "# Using Resource principal to authenticate against the model endpoint \n",
    "auth = oci.auth.signers.get_resource_principals_signer()\n",
    "    \n",
    "\n",
    "response = requests.post(uri, json=payload_json, auth=auth)\n",
    "print(response)\n",
    "\n",
    "print(json.loads(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3cc37ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prediction_deepface': {'verified': 'Same person',\n",
       "  'distance': 0.3519110140477171,\n",
       "  'age_prof': 27,\n",
       "  'gender_prof': 'Man',\n",
       "  'age_pass': 24,\n",
       "  'gender_pass': 'Man'}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f433910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e5d7504",
   "metadata": {},
   "source": [
    "## **Gradio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02a65d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import oci\n",
    "from oci.signer import Signer\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "def full_function(img_1, img_2):\n",
    "    \n",
    "    \n",
    "    payload_string = str()\n",
    "    full_payload_string = str()\n",
    "    counter = 0\n",
    "\n",
    "    for image in [img_1, img_2]:\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "#         input_path=os.path.join(file_path, images)\n",
    "#         print(input_path)\n",
    "\n",
    "        with open(image, \"rb\") as image2string:\n",
    "            converted_string = base64.b64encode(image2string.read()).decode('ascii')\n",
    "\n",
    "            #add payload to full string\n",
    "\n",
    "            if counter == 1:\n",
    "                payload1 = json.dumps(converted_string)\n",
    "                json_payload1 = json.loads(payload1)\n",
    "\n",
    "            else: #if counter is 2\n",
    "                payload2 = json.dumps(converted_string)\n",
    "                json_payload2 = json.loads(payload2)\n",
    "\n",
    "    payload_json = {'data':{'passport_data': json_payload1,'profile_data': json_payload2}}\n",
    "    \n",
    "    \n",
    "\n",
    "    # REST API\n",
    "    uri = f\"https://modeldeployment.eu-frankfurt-1.oci.customer-oci.com/ocid1.datasciencemodeldeployment.oc1.eu-frankfurt-1.amaaaaaangencdya3p3wmufctj23w3t4o25xla77eryywml7q2oti6sai7cq/predict\"\n",
    "\n",
    "    # Using Resource principal to authenticate against the model endpoint \n",
    "    auth = oci.auth.signers.get_resource_principals_signer()\n",
    "\n",
    "\n",
    "    response = requests.post(uri, json=payload_json, auth=auth)\n",
    "    \n",
    "    x = json.loads(response.content)\n",
    "\n",
    "    result_person = x['prediction_deepface']['verified']\n",
    "    age_img_1 = x['prediction_deepface']['age_prof']\n",
    "    gender_img_1 = x['prediction_deepface']['gender_prof']\n",
    "    age_img_2 = x['prediction_deepface']['age_pass']\n",
    "    gender_img_2 = x['prediction_deepface']['gender_pass']\n",
    "    \n",
    "\n",
    "    return result_person, age_img_1, gender_img_1, age_img_2, gender_img_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a9072a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://aef4e977479340a6e0.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://aef4e977479340a6e0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "desc = \"Compare persons!\"\n",
    "\n",
    "with gr.Blocks() as demo: \n",
    "     \n",
    "    img_1 = gr.Image(type=\"filepath\", info=\"Upload image 1\")\n",
    "    img_2 = gr.Image(type=\"filepath\", info=\"Upload image 2\")\n",
    "\n",
    "    result_person = gr.Text(label=\"Result from DeepFace\")\n",
    "    age_img_1 = gr.Text(label=\"Predicted age image 1\")\n",
    "    gender_img_1 = gr.Text(label=\"Predicted gender image 1\")\n",
    "    age_img_2 = gr.Text(label=\"Predicted age image 2\")\n",
    "    gender_img_2 = gr.Text(label=\"Predicted gender image 2\")\n",
    "\n",
    "    submit_btn = gr.Button(\"Run DeepFace\")\n",
    "\n",
    "\n",
    "gr.Interface(fn=full_function, inputs=[img_1, img_2], outputs=[result_person, age_img_1, gender_img_1, age_img_2, gender_img_2], title=desc).launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3fbe18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff881d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b2a93a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4287c13c",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fceedd1",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c5e74e",
   "metadata": {},
   "source": [
    "# **Overwriting the deep face files to load model locally, which are stored in the artifacts**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a106ac",
   "metadata": {},
   "source": [
    "## **Change the .py file that tries to get the model to locall directory**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cd13c0",
   "metadata": {},
   "source": [
    "The below slightly changes the .py files that is in the Deepface library. So that it loads the model from local directory and not tries to load from hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ab2247",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile \"/home/datascience/conda/tensorflow28_p38_gpu_v1/lib/python3.8/site-packages/deepface/basemodels/VGGFace.py\"\n",
    "\n",
    "import os\n",
    "import gdown\n",
    "import tensorflow as tf\n",
    "from deepface.commons import functions\n",
    "\n",
    "# ---------------------------------------\n",
    "\n",
    "tf_version = int(tf.__version__.split(\".\", maxsplit=1)[0])\n",
    "\n",
    "if tf_version == 1:\n",
    "    from keras.models import Model, Sequential\n",
    "    from keras.layers import (\n",
    "        Convolution2D,\n",
    "        ZeroPadding2D,\n",
    "        MaxPooling2D,\n",
    "        Flatten,\n",
    "        Dropout,\n",
    "        Activation,\n",
    "    )\n",
    "else:\n",
    "    from tensorflow.keras.models import Model, Sequential\n",
    "    from tensorflow.keras.layers import (\n",
    "        Convolution2D,\n",
    "        ZeroPadding2D,\n",
    "        MaxPooling2D,\n",
    "        Flatten,\n",
    "        Dropout,\n",
    "        Activation,\n",
    "    )\n",
    "\n",
    "# ---------------------------------------\n",
    "\n",
    "\n",
    "def baseModel():\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1, 1), input_shape=(224, 224, 3)))\n",
    "    model.add(Convolution2D(64, (3, 3), activation=\"relu\"))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(64, (3, 3), activation=\"relu\"))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(128, (3, 3), activation=\"relu\"))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(128, (3, 3), activation=\"relu\"))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, (3, 3), activation=\"relu\"))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, (3, 3), activation=\"relu\"))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, (3, 3), activation=\"relu\"))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation=\"relu\"))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation=\"relu\"))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation=\"relu\"))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation=\"relu\"))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation=\"relu\"))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation=\"relu\"))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(Convolution2D(4096, (7, 7), activation=\"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Convolution2D(4096, (1, 1), activation=\"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Convolution2D(2622, (1, 1)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Activation(\"softmax\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# url = 'https://drive.google.com/uc?id=1CPSeum3HpopfomUEK1gybeuIVoeJT_Eo'\n",
    "\n",
    "\n",
    "def loadModel(\n",
    "    url=\"https://github.com/serengil/deepface_models/releases/download/v1.0/vgg_face_weights.h5\",\n",
    "):\n",
    "\n",
    "    model = baseModel()\n",
    "\n",
    "    # -----------------------------------\n",
    "\n",
    "    home = functions.get_deepface_home()\n",
    "    output = \"./vgg_face_weights.h5\"                                   ################# change this path!!!!! ################# ####################################################################\n",
    "    # locally: \"./model_artifacts/.deepface/weights/vgg_face_weights.h5\" \n",
    "    # original: output = home + \"/.deepface/weights/vgg_face_weights.h5\"\n",
    "    # in model deployment: \"./vgg_face_weights.h5\" \n",
    "\n",
    "    if os.path.isfile(output) != True:\n",
    "        print(\"vgg_face_weights.h5 will be downloaded...\")\n",
    "        gdown.download(url, output, quiet=False)\n",
    "\n",
    "    # -----------------------------------\n",
    "\n",
    "    model.load_weights(output)\n",
    "\n",
    "    # -----------------------------------\n",
    "\n",
    "    # TO-DO: why?\n",
    "    vgg_face_descriptor = Model(inputs=model.layers[0].input, outputs=model.layers[-2].output)\n",
    "\n",
    "    return vgg_face_descriptor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c09004c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile \"/home/datascience/conda/tensorflow28_p38_gpu_v1/lib/python3.8/site-packages/deepface/extendedmodels/Age.py\"\n",
    "\n",
    "import os\n",
    "import gdown\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from deepface.basemodels import VGGFace\n",
    "from deepface.commons import functions\n",
    "\n",
    "# ----------------------------------------\n",
    "# dependency configurations\n",
    "\n",
    "tf_version = int(tf.__version__.split(\".\", maxsplit=1)[0])\n",
    "\n",
    "if tf_version == 1:\n",
    "    from keras.models import Model, Sequential\n",
    "    from keras.layers import Convolution2D, Flatten, Activation\n",
    "elif tf_version == 2:\n",
    "    from tensorflow.keras.models import Model, Sequential\n",
    "    from tensorflow.keras.layers import Convolution2D, Flatten, Activation\n",
    "\n",
    "# ----------------------------------------\n",
    "\n",
    "\n",
    "def loadModel(\n",
    "    url=\"https://github.com/serengil/deepface_models/releases/download/v1.0/age_model_weights.h5\",\n",
    "):\n",
    "\n",
    "    model = VGGFace.baseModel()\n",
    "\n",
    "    # --------------------------\n",
    "\n",
    "    classes = 101\n",
    "    base_model_output = Sequential()\n",
    "    base_model_output = Convolution2D(classes, (1, 1), name=\"predictions\")(model.layers[-4].output)\n",
    "    base_model_output = Flatten()(base_model_output)\n",
    "    base_model_output = Activation(\"softmax\")(base_model_output)\n",
    "\n",
    "    # --------------------------\n",
    "\n",
    "    age_model = Model(inputs=model.input, outputs=base_model_output)\n",
    "\n",
    "    # --------------------------\n",
    "\n",
    "    # load weights\n",
    "\n",
    "#     home = functions.get_deepface_home()\n",
    "\n",
    "#     if os.path.isfile(home + \"/.deepface/weights/age_model_weights.h5\") != True:\n",
    "#         print(\"age_model_weights.h5 will be downloaded...\")\n",
    "\n",
    "#         output = home + \"/.deepface/weights/age_model_weights.h5\"\n",
    "#         gdown.download(url, output, quiet=False)\n",
    "\n",
    "    age_model.load_weights(\"./age_model_weights.h5\")\n",
    "\n",
    "    return age_model\n",
    "\n",
    "    # --------------------------\n",
    "\n",
    "\n",
    "def findApparentAge(age_predictions):\n",
    "    output_indexes = np.array(list(range(0, 101)))\n",
    "    apparent_age = np.sum(age_predictions * output_indexes)\n",
    "    return apparent_age\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aab4726",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile \"/home/datascience/conda/tensorflow28_p38_gpu_v1/lib/python3.8/site-packages/deepface/extendedmodels/Gender.py\"\n",
    "\n",
    "import os\n",
    "import gdown\n",
    "import tensorflow as tf\n",
    "from deepface.basemodels import VGGFace\n",
    "from deepface.commons import functions\n",
    "\n",
    "# -------------------------------------\n",
    "# pylint: disable=line-too-long\n",
    "# -------------------------------------\n",
    "# dependency configurations\n",
    "\n",
    "tf_version = int(tf.__version__.split(\".\", maxsplit=1)[0])\n",
    "\n",
    "if tf_version == 1:\n",
    "    from keras.models import Model, Sequential\n",
    "    from keras.layers import Convolution2D, Flatten, Activation\n",
    "elif tf_version == 2:\n",
    "    from tensorflow.keras.models import Model, Sequential\n",
    "    from tensorflow.keras.layers import Convolution2D, Flatten, Activation\n",
    "# -------------------------------------\n",
    "\n",
    "# Labels for the genders that can be detected by the model.\n",
    "labels = [\"Woman\", \"Man\"]\n",
    "\n",
    "\n",
    "def loadModel(\n",
    "    url=\"https://github.com/serengil/deepface_models/releases/download/v1.0/gender_model_weights.h5\",\n",
    "):\n",
    "\n",
    "    model = VGGFace.baseModel()\n",
    "\n",
    "    # --------------------------\n",
    "\n",
    "    classes = 2\n",
    "    base_model_output = Sequential()\n",
    "    base_model_output = Convolution2D(classes, (1, 1), name=\"predictions\")(model.layers[-4].output)\n",
    "    base_model_output = Flatten()(base_model_output)\n",
    "    base_model_output = Activation(\"softmax\")(base_model_output)\n",
    "\n",
    "    # --------------------------\n",
    "\n",
    "    gender_model = Model(inputs=model.input, outputs=base_model_output)\n",
    "\n",
    "    # --------------------------\n",
    "\n",
    "    # load weights\n",
    "\n",
    "#     home = functions.get_deepface_home()\n",
    "\n",
    "#     if os.path.isfile(home + \"/.deepface/weights/gender_model_weights.h5\") != True:\n",
    "#         print(\"gender_model_weights.h5 will be downloaded...\")\n",
    "\n",
    "#         output = home + \"/.deepface/weights/gender_model_weights.h5\"\n",
    "#         gdown.download(url, output, quiet=False)\n",
    "\n",
    "    gender_model.load_weights(\"./gender_model_weights.h5\")\n",
    "\n",
    "    return gender_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484f270b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f20aa0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0357ace5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow28_p38_gpu_v1]",
   "language": "python",
   "name": "conda-env-tensorflow28_p38_gpu_v1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
