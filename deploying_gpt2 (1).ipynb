{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2490b3f",
   "metadata": {},
   "source": [
    "# **GPT2- Deploying model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b781db3",
   "metadata": {},
   "source": [
    "## **Load pre-trained model and save model locally**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4df352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TFAutoModelForSequenceClassification, AutoTokenizer, TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "from scipy.special import softmax\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d1c001",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git lfs install\n",
    "!git clone https://huggingface.co/gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f28fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_gpt2 = \"./gpt2\"\n",
    "\n",
    "model = TFGPT2LMHeadModel.from_pretrained(dir_gpt2)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(dir_gpt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64af88e3",
   "metadata": {},
   "source": [
    "## **Make prediction from locally loaded model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a32584",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Please finnish my sentence, I'm going home and I will \"\n",
    "#text = preprocess(text)\n",
    "input_ids = tokenizer.encode(text, return_tensors='tf')\n",
    "\n",
    "greedy_output = model.generate(input_ids, max_length=50)\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e78625",
   "metadata": {},
   "source": [
    "## **Local prediction again. Serialized as json input. Needed for when deployed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9cec3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make post request\n",
    "data = {\"text\": \"Hi, how you doing today\"}\n",
    "\n",
    "#fetch text from input\n",
    "text_input = data['text']\n",
    "\n",
    "\n",
    "# same script again\n",
    "input_ids = tokenizer.encode(text_input, return_tensors='tf')\n",
    "\n",
    "greedy_output = model.generate(input_ids, max_length=50)\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "results = tokenizer.decode(greedy_output[0], skip_special_tokens=True)\n",
    "print(type(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669f345f",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eedfc17",
   "metadata": {},
   "source": [
    "# **Create model artifacts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938c061b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ads.model.framework.tensorflow_model import TensorFlowModel\n",
    "from ads.common.model_metadata import UseCaseType\n",
    "from ads.common.model_artifact import ModelArtifact\n",
    "from ads.common.model_export_util import prepare_generic_model\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2faf9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_artifacts = './testgpt2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164678e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to artifacts and conda slug\n",
    "path_to_artifacts = './testgpt2'\n",
    "#conda_env = 'oci://conda_environment_yolov5@frqap2zhtzbe/conda_environments/cpu/fdf_conda/1.0/fdf_conda'   #this refers to the published conda location (bucket name, namespace)\n",
    "\n",
    "#create default artifacts\n",
    "artifact = prepare_generic_model(\n",
    "    path_to_artifacts, \n",
    "    fn_artifact_files_included=False, \n",
    "    force_overwrite=True, \n",
    "    inference_conda_env=\"tensorflow28_p38_gpu_v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b403e52",
   "metadata": {},
   "source": [
    "# **Change the score.py manually**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7cdc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy all files in the model artifacts\n",
    "!cp -a ./gpt2 ./testgpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164c9f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile \"{path_to_artifacts}/score.py\"\n",
    "import os\n",
    "import ads\n",
    "from transformers import AutoModelForSequenceClassification, TFAutoModelForSequenceClassification, AutoTokenizer, TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tokenizers\n",
    "import json\n",
    "import os\n",
    "\n",
    "#load model and tokenizer\n",
    "model_artifacts_folder = \"./gpt2\"\n",
    "\n",
    "def load_model():\n",
    "    class DummyModel:\n",
    "        def __init__(self):\n",
    "            pass\n",
    "    return DummyModel()\n",
    "\n",
    "#loading the model before seemed to fail. NOt sure why. Now loading in the predict.\n",
    "\n",
    "def predict(data, model=load_model()):       \n",
    "\n",
    "    model = TFGPT2LMHeadModel.from_pretrained(model_artifacts_folder)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_artifacts_folder)\n",
    "    \n",
    "    #fetch text from input\n",
    "    text_input = data['text']\n",
    "    \n",
    "    \n",
    "    # process text\n",
    "    input_ids = tokenizer.encode(text_input, return_tensors='tf')\n",
    "    greedy_output = model.generate(input_ids, max_length=50)\n",
    "    results = tokenizer.decode(greedy_output[0], skip_special_tokens=True)\n",
    "    return {'prediction': results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4212b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"text\": \"Hi, Thanks, i'm very sad. You such\"}\n",
    "\n",
    "predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c6969a",
   "metadata": {},
   "source": [
    "## **check the artifacts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ef9d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Bob uses GPU vs CPU conda env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5918abb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile \"{path_to_artifacts}/runtime.yaml\"\n",
    "\n",
    "# Model runtime environment\n",
    "MODEL_ARTIFACT_VERSION: '3.0'\n",
    "MODEL_DEPLOYMENT:\n",
    "  INFERENCE_CONDA_ENV:\n",
    "    INFERENCE_ENV_PATH: oci://service-conda-packs@id19sfcrra6z/service_pack/gpu/TensorFlow_2.8_for_GPU_on_Python_3.8/1.0/tensorflow28_p38_gpu_v1\n",
    "    INFERENCE_ENV_SLUG: tensorflow28_p38_gpu_v1\n",
    "    INFERENCE_ENV_TYPE: data_science\n",
    "    INFERENCE_PYTHON_VERSION: '3.8'\n",
    "MODEL_PROVENANCE:\n",
    "  PROJECT_OCID: ocid1.datascienceproject.oc1.eu-frankfurt-1.amaaaaaangencdyaik5ssdqk4as2bhldxprh7vnqpk7yycsm7vymd344cgua\n",
    "  TENANCY_OCID: ocid1.tenancy.oc1..aaaaaaaabu5fgingcjq3vc7djuwsdcutdxs4gsws6h4kfoldqpjuggxprgoa\n",
    "  TRAINING_COMPARTMENT_OCID: ocid1.compartment.oc1..aaaaaaaae3n6r6hrjipbap2hojicrsvkzatrtlwvsyrpyjd7wjnw4za3m75q\n",
    "  TRAINING_CONDA_ENV:\n",
    "    TRAINING_ENV_PATH: oci://service-conda-packs@id19sfcrra6z/service_pack/gpu/TensorFlow_2.8_for_GPU_on_Python_3.8/1.0/tensorflow28_p38_gpu_v1\n",
    "    TRAINING_ENV_SLUG: tensorflow28_p38_gpu_v1\n",
    "    TRAINING_ENV_TYPE: data_science\n",
    "    TRAINING_PYTHON_VERSION: '3.8'\n",
    "  TRAINING_REGION: eu-frankfurt-1\n",
    "  TRAINING_RESOURCE_OCID: ocid1.datasciencenotebooksession.oc1.eu-frankfurt-1.amaaaaaangencdyacxmsz5ycch762wjc54udhibtl3m4nacuaf7shrvyoktq\n",
    "  USER_OCID: ocid1.saml2idp.oc1..aaaaaaaar3ydw5hoiob7dfjzoom2dvbhqkkd5fat6m7upe72emlsxhsfrbfa/bob.peulen@oracle.com\n",
    "  VM_IMAGE_INTERNAL_ID: NB1480-DCGPU131-VMP64-VMA1585-BI681"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deba627c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all should be passed\n",
    "artifact.introspect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96491c84",
   "metadata": {},
   "source": [
    "## **Test model from artifacts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabebe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, path_to_artifacts)\n",
    " \n",
    "# importing load_model() and predict() that are defined in score.py\n",
    "from score import load_model, predict\n",
    " \n",
    "# Loading the model to memory\n",
    "_ = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1bf796",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"text\": \"Hi, Thanks, i'm very sad. You such\"}\n",
    "\n",
    "predictions_test = predict(data, _)\n",
    "predictions_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a52bd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /home/datascience/.oci/config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eee6d96",
   "metadata": {},
   "source": [
    "## **store in catalog**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cfe33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import oci\n",
    "config = oci.config.from_file('/home/datascience/.oci/config', 'DEFAULT')\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e9890d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### deleted some files (tf.. differnet bits files) to reduce to < 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1386d911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                         27.0MB/s]\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop1:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'ocid1.datasciencemodel.oc1.eu-frankfurt-1.amaaaaaangencdyauanowud3tyib2c627wqd6c5toehvcmywsfjeczx4wgja'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Saving the model artifact to the model catalog. \n",
    "catalog_entry = artifact.save(display_name='gptsaitest', description='gptsaitest', timeout=600, bucket_uri = \"oci://conda_environment_yolov5@frqap2zhtzbe/model_artifacts_large\")\n",
    "catalog_entry.id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfb0562",
   "metadata": {},
   "source": [
    "## **Deploy in the UI**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277bcc77",
   "metadata": {},
   "source": [
    "## **Invoke the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42e89d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import oci\n",
    "from oci.signer import Signer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f053a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://modeldeployment.eu-frankfurt-1.oci.customer-oci.com/ocid1.datasciencemodeldeployment.oc1.eu-frankfurt-1.amaaaaaangencdyasl6dqv3i7lqwh5zhsgamx6n64h5qbjwafqbb7k3nwpla/predict\n"
     ]
    }
   ],
   "source": [
    "#fdf version mf_final\n",
    "uri = f\"https://modeldeployment.eu-frankfurt-1.oci.customer-oci.com/ocid1.datasciencemodeldeployment.oc1.eu-frankfurt-1.amaaaaaangencdyasl6dqv3i7lqwh5zhsgamx6n64h5qbjwafqbb7k3nwpla/predict\"\n",
    "print(uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a7f2521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Resource principal to authenticate against the model endpoint. Set using_rps=False if you are using the config+key flow. \n",
    "using_rps = False\n",
    "\n",
    "if using_rps: # using resource principal:     \n",
    "    auth = oci.auth.signers.get_resource_principals_signer()\n",
    "else: # using config + key: \n",
    "    config = oci.config.from_file(\"/home/datascience/.oci/config\") # replace with the location of your oci config file\n",
    "    auth = Signer(\n",
    "        tenancy=config['tenancy'],\n",
    "        user=config['user'],\n",
    "        fingerprint=config['fingerprint'],\n",
    "        private_key_file_location=config['key_file'],\n",
    "        pass_phrase=config['pass_phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3b5ee1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<oci.signer.Signer object at 0x7f1d5957e7c0>\n",
      "<Response [200]>\n",
      "{'prediction': \"I had breakfast this morning, and now I'm going to \\xa0have a little bit of a break. I'm going to go to the gym and do some work. I'm going to go to the gym and do some work. I'm\"}\n",
      "I had breakfast this morning, and now I'm going to  have a little bit of a break. I'm going to go to the gym and do some work. I'm going to go to the gym and do some work. I'm\n",
      "CPU times: user 16.3 ms, sys: 1.19 ms, total: 17.5 ms\n",
      "Wall time: 5.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import json\n",
    "print(auth)\n",
    "#input data\n",
    "data = {\"text\": \"I had breakfast this morning, and now I'm going to \"}\n",
    "\n",
    "#POST request to the model\n",
    "response = requests.post(uri, json=data, auth=auth)\n",
    "print(response)\n",
    "xx = (json.loads(response.content))\n",
    "print(xx)\n",
    "print(xx['prediction'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad5b2db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4911079",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow28_p38_gpu_v1]",
   "language": "python",
   "name": "conda-env-tensorflow28_p38_gpu_v1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
