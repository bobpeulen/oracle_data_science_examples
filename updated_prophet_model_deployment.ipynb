{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b44c406d",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca29f520",
   "metadata": {},
   "source": [
    "<img src=\"data:image/png;base64,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\" style=\"height:150px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b4d972",
   "metadata": {},
   "source": [
    "# **Anomaly Detection & Forecasting**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673dd96e",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72874b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To do\n",
    "# send e-mail if something wrong?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d20591",
   "metadata": {},
   "source": [
    "## **1. Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd1c906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import pickle\n",
    "from prophet import Prophet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2fd42b",
   "metadata": {},
   "source": [
    "## **2. Load and read file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d741e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inputx = pd.read_csv(\"input.csv\")\n",
    "df_inputx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fdb951",
   "metadata": {},
   "outputs": [],
   "source": [
    "### filter out EXTSPT01\n",
    "df_input = df_inputx[df_inputx['EXTRACT']=='EXTSPT02']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf5bdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#size of df\n",
    "print(\"Shape of Dataframe\" + str(df_input.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670395ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the data type per column\n",
    "df_input.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc520cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#review unique values\n",
    "print(df_input['EXTRACT'].unique())\n",
    "print(df_input['DATA_PUMP'].unique())\n",
    "print(df_input['REPLICAT'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83e83c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#review input data\n",
    "df_input.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0317b1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inputx = df_input.head(1000)\n",
    "xpoints = df_inputx['SOURCE_HB_TS']\n",
    "ypoints = df_inputx['REPLICAT_READ_LAG']\n",
    "\n",
    "plt.figure().set_figwidth(15)\n",
    "plt.plot(xpoints, ypoints, linestyle = 'dotted')\n",
    "plt.xlabel('TIME')\n",
    "plt.ylabel('REPLICAT_READ_LAG')\n",
    "\n",
    "#plt.xticks(rotation=45)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f40ad31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inputx = df_input.head(10000)\n",
    "xpoints = df_inputx['SOURCE_HB_TS']\n",
    "ypoints = df_inputx['EXTRACT_LAG']\n",
    "\n",
    "plt.figure().set_figwidth(15)\n",
    "plt.plot(xpoints, ypoints, linestyle = 'dotted')\n",
    "plt.xlabel('TIME')\n",
    "plt.ylabel('EXTRACT_LAG')\n",
    "\n",
    "#plt.xticks(rotation=45)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15331d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_inputx = df_input.head(10000)\n",
    "# xpoints = df_inputx['SOURCE_HB_TS']\n",
    "# ypoints = df_inputx['DATA_PUMP_READ_LAG']\n",
    "\n",
    "# plt.figure().set_figwidth(15)\n",
    "# plt.plot(xpoints, ypoints, linestyle = 'dotted')\n",
    "# plt.xlabel('TIME')\n",
    "# plt.ylabel('DATA_PUMP_READ_LAG')\n",
    "\n",
    "# #plt.xticks(rotation=45)\n",
    "\n",
    "# # Display the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2362f981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_inputx = df_input.head(10000)\n",
    "# xpoints = df_inputx['SOURCE_HB_TS']\n",
    "# ypoints = df_inputx['REPLICAT_READ_LAG']\n",
    "\n",
    "# plt.figure().set_figwidth(15)\n",
    "# plt.plot(xpoints, ypoints, linestyle = 'dotted')\n",
    "# plt.xlabel('TIME')\n",
    "# plt.ylabel('REPLICAT_READ_LAG')\n",
    "\n",
    "# #plt.xticks(rotation=45)\n",
    "\n",
    "# # Display the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce47267",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fafbb0c",
   "metadata": {},
   "source": [
    "# **3. Data Prep**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1562c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only get 5 individual columns\n",
    "df_input_kpi = df_input[['SOURCE_HB_TS', 'EXTRACT_LAG', 'DATA_PUMP_READ_LAG', 'REPLICAT_READ_LAG', 'REPLICAT_APPLY_LAG', 'TOTAL_LAG']]\n",
    "\n",
    "#convert object to datetime\n",
    "df_input_kpi['SOURCE_HB_TS'] = pd.to_datetime(df_input_kpi['SOURCE_HB_TS'])\n",
    "df_input_kpi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8b40ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check datatime. First column should be datetime\n",
    "df_input_kpi.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2e63d8",
   "metadata": {},
   "source": [
    "# **4. ML models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da53a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_minutes_used_in_training = 60  #30 minutes used for training\n",
    "forecast_in_minutes = 10  #10 minutes will be forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc9b9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataframe ends at \" + str(df_input_kpi.tail(1).SOURCE_HB_TS))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835c14ac",
   "metadata": {},
   "source": [
    "## Model 1 - FB Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400d2f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter on columsn and change name\n",
    "df_input_extract_lag = df_input[['SOURCE_HB_TS', 'TOTAL_LAG']]\n",
    "\n",
    "df_input_extract_lag.rename(columns={\"SOURCE_HB_TS\": \"ds\", \"TOTAL_LAG\":\"y\"}, inplace=True)\n",
    "df_input_extract_lag['ds'] = pd.to_datetime(df_input_extract_lag['ds'])\n",
    "\n",
    "#filter on X last minutes for forecast\n",
    "input_prophet_1 = df_input_extract_lag.tail(last_minutes_used_in_training)\n",
    "input_prophet_1['ds'] = pd.to_datetime(input_prophet_1['ds'])\n",
    "\n",
    "m_1 = Prophet(changepoint_prior_scale=0.9, daily_seasonality=True)\n",
    "m_1.fit(input_prophet_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f15994",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a empty dataframe with forecast dates\n",
    "future_1 = m_1.make_future_dataframe(periods=forecast_in_minutes, freq=\"min\")\n",
    "\n",
    "#use the model to predict\n",
    "forecast_1 = m_1.predict(future_1)\n",
    "forecast_1[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e0f4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = m_1.plot(forecast_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9928f7",
   "metadata": {},
   "source": [
    "### Use the forecast (10 minutes in the future) to see expected growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50efd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## simple calculation as example\n",
    "\n",
    "# average actual 30 minutes as input\n",
    "avg_input = forecast_1['yhat'].head(30).mean()\n",
    "max_input = forecast_1['yhat'].head(30).max()\n",
    "min_input = forecast_1['yhat'].head(30).min()\n",
    "print(\"Average in input value is \" + str(avg_input))\n",
    "print(\"Max input value \" + str(max_input))\n",
    "print(\"Min input value \" + str(min_input))\n",
    "print(\" ------------------- \")\n",
    "#average 10 minutes as forecasted\n",
    "avg_forecast = forecast_1['yhat'].tail(10).mean()\n",
    "max_forecast = forecast_1['yhat'].tail(10).max() \n",
    "min_forecast = forecast_1['yhat'].tail(10).min() \n",
    "print(\"Average in forecast value is \" + str(avg_forecast))\n",
    "print(\"Max forecast value \" + str(max_forecast))\n",
    "print(\"Min forecast value \" + str(min_forecast))\n",
    "print(\" ------------------- \")\n",
    "\n",
    "#increase/decrease based on actual vs forecasted\n",
    "diff_perc_input_vs_forecast = round(((avg_forecast-avg_input)/avg_input)*100,2)\n",
    "diff_perc_max = round(((max_forecast-max_input)/max_input)*100,2)\n",
    "diff_perc_min = round(((min_forecast-min_input)/min_input)*100,2)\n",
    "print(\"Expected increase or decreases in the coming 10 minutes is \" + str(diff_perc_input_vs_forecast)+str(\"%\"))\n",
    "print(\"Expected increase/decrease in max \"  + str(diff_perc_max)+str(\"%\"))\n",
    "print(\"Expected increase/decrease in min \"  + str(diff_perc_min)+str(\"%\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f542da7f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b3fe8e",
   "metadata": {},
   "source": [
    "# **4. Create model artifacts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0a89b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!odsc conda init -b conda_env -n frzpemb9ufe8 -a resource_principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53527dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!odsc conda publish -s tensorflow28_p38_cpu_v1 --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "80cd2eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ads.model.framework.tensorflow_model import TensorFlowModel\n",
    "from ads.common.model_metadata import UseCaseType\n",
    "from ads.common.model_artifact import ModelArtifact\n",
    "from ads.common.model_export_util import prepare_generic_model\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "50aae3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/tmp/ipykernel_15280/1558954823.py:6: DeprecationWarning: Method prepare_generic_model is deprecated in 2.6.6 and will be removed in a future release. Use framework specific Model utility class for saving and deploying model saving and deploying. Check https://accelerated-data-science.readthedocs.io/en/latest/user_guide/model_registration/quick_start.html\n",
      "  artifact = prepare_generic_model(\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop1:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:ADS:As force_overwrite is set to True, all the existing files in the ./model_artifacts_v8 will be removed\n",
      "WARNING:ADS:Taxonomy metadata was not extracted. To auto-populate taxonomy metadata the model must be provided. Pass the model as a parameter to .prepare_generic_model(model=model, usecase_type=UseCaseType.REGRESSION). Alternative way is using atifact.populate_metadata(model=model, usecase_type=UseCaseType.REGRESSION).\n"
     ]
    }
   ],
   "source": [
    "#path to artifacts and conda slug\n",
    "path_to_artifacts = './model_artifacts_v8'\n",
    "conda_env = 'oci://conda_env@frzpemb9ufe8/conda_environments/cpu/TensorFlow 2.8 for CPU on Python 3.8/1.0/tensorflow28_p38_cpu_v1'  \n",
    "\n",
    "#create default artifacts\n",
    "artifact = prepare_generic_model(\n",
    "    path_to_artifacts, \n",
    "    fn_artifact_files_included=False, \n",
    "    force_overwrite=True, \n",
    "    inference_conda_env=conda_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1ebc1f",
   "metadata": {},
   "source": [
    "# **5. Full code score.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "01c4163d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./model_artifacts_v8/score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./model_artifacts_v8/score.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import uuid\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import pickle\n",
    "import gzip\n",
    "from prophet import Prophet\n",
    "import ads\n",
    "import os\n",
    "import configparser\n",
    "import shutil\n",
    "from zipfile import ZipFile\n",
    "from tempfile import NamedTemporaryFile\n",
    "import urllib\n",
    "import re\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "import cx_Oracle\n",
    "from ocifs import OCIFileSystem\n",
    "import cx_Oracle\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    class DummyModel:\n",
    "        def __init__(self):\n",
    "            pass\n",
    "    return DummyModel()\n",
    "\n",
    "#create folder for input files\n",
    "if not os.path.exists(\"input_files\"):\n",
    "    os.makedirs(\"input_files\")\n",
    "\n",
    "############################\n",
    "############################\n",
    "\n",
    "\n",
    "list_variables = ['EXTRACT_LAG', 'DATA_PUMP_READ_LAG', 'REPLICAT_READ_LAG', 'REPLICAT_APPLY_LAG', 'TOTAL_LAG']\n",
    "\n",
    "\n",
    "\n",
    "def predict(data, model=load_model()):\n",
    "    \n",
    "    #test print root\n",
    "    print('Get current working directory : ', os.getcwd())\n",
    "       \n",
    "    input_files_location = os.getcwd() + \"/input_files/\"\n",
    "    print(\"Input file full location \" + input_files_location)\n",
    "\n",
    "    #get the bucket name, namespace, and full file name\n",
    "    file_name = data['file_name']\n",
    "    bucket_name = data['bucket_name']\n",
    "    namespace = data['namespace']\n",
    "    \n",
    "    #get full location in bucket\n",
    "    full_location_in_bucket = \"oci://\" + bucket_name + \"@\" + namespace + \"/LagMetrics/ocid1.serviceconnector.oc1.eu-frankfurt-1.amaaaaaapixtsjiarpunoxhi6tvcw3jdbgwyt6xzv4cl4zryhuigxesyyjmq/\"+file_name\n",
    "    print(\"full location in bucket \" + full_location_in_bucket)\n",
    "       \n",
    "    raw_input_from_zip = pd.read_csv(full_location_in_bucket, names=['SOURCE_HB_TS','EXTRACT', 'EXTRACT_LAG','DATA_PUMP','DATA_PUMP_READ_LAG','REPLICAT','REPLICAT_READ_LAG','REPLICAT_APPLY_LAG','TOTAL_LAG'], header=None)\n",
    "    \n",
    "    #read the .txt file\n",
    "    input_all_minutes = raw_input_from_zip\n",
    "    print(\"first line of data\" + str(input_all_minutes.head(1)))\n",
    "    \n",
    "    #get the latest 60 minues only.\n",
    "    input_60_minutes = input_all_minutes.tail(60)\n",
    "    print(input_60_minutes.shape)\n",
    "    \n",
    "    #add random id for id set\n",
    "    set_id = \"set_id_\"+ str(uuid.uuid4())\n",
    "    \n",
    "        \n",
    "    #loop through the 5 variables. Build forecast for each one of them and push to database\n",
    "    for variable in list_variables:\n",
    "        \n",
    "        print(\"-------------------------------------------------------\")\n",
    "        print(\"Start variable \" + variable)\n",
    "        print(type(variable))\n",
    "        \n",
    "        #create empty list\n",
    "        list_to_db =  []\n",
    "    \n",
    "        #only select one variable to filter on\n",
    "        df_input_1_variable = input_60_minutes[['SOURCE_HB_TS', variable]]\n",
    "        \n",
    "        #change name to what Prophet expects       \n",
    "        df_input_1_variable.rename(columns={\"SOURCE_HB_TS\": \"ds\", variable:\"y\"}, inplace=True)\n",
    "        \n",
    "        #convert ds to timeframe\n",
    "        df_input_1_variable['ds'] = pd.to_datetime(df_input_1_variable['ds'])\n",
    "        \n",
    "        #start date\n",
    "        start_date = df_input_1_variable['ds'].min()\n",
    "        print(\"Start time of \" + variable + \" is \" + str(start_date))\n",
    "\n",
    "        #end date\n",
    "        end_date = df_input_1_variable['ds'].max()\n",
    "        print(\"End time of \" + variable + \" is \" + str(end_date))\n",
    "\n",
    "        #general settings\n",
    "        forecast_in_minutes = 10  #10 minutes will be forecast\n",
    "\n",
    "        m_1 = Prophet(changepoint_prior_scale=0.9)\n",
    "        m_1.fit(df_input_1_variable)\n",
    "\n",
    "        #create a empty dataframe with forecast dates\n",
    "        future_1 = m_1.make_future_dataframe(periods=forecast_in_minutes, freq=\"min\")\n",
    "\n",
    "        #use the model to predict\n",
    "        forecast_1 = m_1.predict(future_1)\n",
    "        forecast_1[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()\n",
    "\n",
    "        # average actual 60 minutes as input\n",
    "        avg_input = forecast_1['yhat'].head(60).mean()\n",
    "        max_input = forecast_1['yhat'].head(60).max()\n",
    "        min_input = forecast_1['yhat'].head(60).min()\n",
    "        print(\"Average in input value is \" + str(avg_input))\n",
    "        print(\"Max input value \" + str(max_input))\n",
    "        print(\"Min input value \" + str(min_input))\n",
    "        print(\" ------------------- \")\n",
    "\n",
    "        #average 10 minutes as forecasted\n",
    "        avg_forecast = forecast_1['yhat'].tail(10).mean()\n",
    "        max_forecast = forecast_1['yhat'].tail(10).max() \n",
    "        min_forecast = forecast_1['yhat'].tail(10).min() \n",
    "        print(\"Average in forecast value is \" + str(avg_forecast))\n",
    "        print(\"Max forecast value \" + str(max_forecast))\n",
    "        print(\"Min forecast value \" + str(min_forecast))\n",
    "        print(\" ------------------- \")\n",
    "\n",
    "        #increase/decrease based on actual vs forecasted\n",
    "        diff_perc_avg = round(((avg_forecast-avg_input)/avg_input)*100,2)\n",
    "        diff_perc_max = round(((max_forecast-max_input)/max_input)*100,2)\n",
    "        diff_perc_min = round(((min_forecast-min_input)/min_input)*100,2)\n",
    "        print(\"Expected increase or decreases in the coming 10 minutes is \" + str(diff_perc_avg)+str(\"%\"))\n",
    "        print(\"Expected increase/decrease in max \"  + str(diff_perc_max)+str(\"%\"))\n",
    "        print(\"Expected increase/decrease in min \"  + str(diff_perc_min)+str(\"%\"))\n",
    "        \n",
    "        \n",
    "        #add to list\n",
    "        list_to_db.append([set_id, variable, start_date, end_date, avg_input, max_input, min_input, avg_forecast, max_forecast, min_forecast, diff_perc_avg, diff_perc_max, diff_perc_min])\n",
    "        df_to_db = pd.DataFrame(list_to_db, columns =['set_id', 'variable', 'start_date', 'end_date', 'avg_input', 'max_input', 'min_input','avg_forecast', 'max_forecast', 'min_forecast', 'diff_perc_avg', 'diff_perc_max', 'diff_perc_min'])\n",
    "\n",
    "        #################\n",
    "        ################ push results to adw\n",
    "       \n",
    "        connection_parameters = {\n",
    "        'user_name': 'OMLUSER',\n",
    "        'password': 'WElcome11POC##',\n",
    "        'service_name': 'pocdb_high',\n",
    "        'wallet_location': \"/home/datascience/model-server/app/deployed_model/credentials/Wallet_pocdb.zip\",\n",
    "        }\n",
    "\n",
    "        \n",
    "        ## push results to database\n",
    "        df_to_db.ads.to_sql('maersk_logs_v3', connection_parameters=connection_parameters, if_exists=\"append\")\n",
    "                            \n",
    "                            \n",
    "                            #, dtype={\n",
    "                \n",
    "#                 'variable': sqlalchemy.types.NVARCHAR(length = 500),\n",
    "#                 'start_date': sqlalchemy.types.DateTime(),\n",
    "#                 'end_date': sqlalchemy.types.DateTime(),\n",
    "#                 'avg_input': sqlalchemy.types.FLOAT(),\n",
    "#                 'max_input': sqlalchemy.types.FLOAT(),\n",
    "#                 'min_input': sqlalchemy.types.FLOAT(),\n",
    "#                 'avg_forecast': sqlalchemy.types.FLOAT(),\n",
    "#                 'max_forecast': sqlalchemy.types.FLOAT(),            \n",
    "#                 'min_forecast': sqlalchemy.types.FLOAT(),            \n",
    "#                 'diff_perc_avg': sqlalchemy.types.FLOAT(),\n",
    "#                 'diff_perc_max': sqlalchemy.types.FLOAT(),\n",
    "#                 'diff_perc_min': sqlalchemy.types.FLOAT()})\n",
    "\n",
    "        #delete list for next loop\n",
    "        del list_to_db\n",
    "        \n",
    "        print()\n",
    "        print(\"-----------------------\")\n",
    "        print(\"Table updated with results for \" + variable)\n",
    "        print(\"-----------------------\")\n",
    "    \n",
    "    \n",
    "    #return {'diff_perc_input_vs_forecast':diff_perc_input_vs_forecast, 'avg_input':avg_input, 'avg_forecast':avg_forecast}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f7d40a",
   "metadata": {},
   "source": [
    "# **6. Test the full code with JSON as input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "acd057b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #example input\n",
    "\n",
    "data = {'file_name':'20231017T103618Z_20231017T104258Z.0.log.gz', 'bucket_name':'LagMetricFiles', 'namespace':'frzpemb9ufe8'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "8daf21d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get current working directory :  /home/datascience\n",
      "Input file full location /home/datascience/input_files/\n",
      "full location in bucket oci://LagMetricFiles@frzpemb9ufe8/LagMetrics/ocid1.serviceconnector.oc1.eu-frankfurt-1.amaaaaaapixtsjiarpunoxhi6tvcw3jdbgwyt6xzv4cl4zryhuigxesyyjmq/20231017T103618Z_20231017T104258Z.0.log.gz\n",
      "first line of data                SOURCE_HB_TS   EXTRACT  EXTRACT_LAG  \\\n",
      "0  22-SEP-23 15:27:30.013717  EXTSPT01     3.118273   \n",
      "\n",
      "                      DATA_PUMP  DATA_PUMP_READ_LAG REPLICAT  \\\n",
      "0  scrbgcsdk011660:9021:SPTFC01            0.523514    RSPT1   \n",
      "\n",
      "   REPLICAT_READ_LAG  REPLICAT_APPLY_LAG  TOTAL_LAG  \n",
      "0           0.281207            0.146456    4.06945  \n",
      "(60, 9)\n",
      "-------------------------------------------------------\n",
      "Start variable EXTRACT_LAG\n",
      "<class 'str'>\n",
      "Start time of EXTRACT_LAG is 2023-09-23 09:12:30.037776\n",
      "End time of EXTRACT_LAG is 2023-09-23 09:42:30.010488\n",
      "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
      "INFO:prophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
      "DEBUG:cmdstanpy:input tempfile: /tmp/tmpj7eup78t/i522wqp9.json\n",
      "DEBUG:cmdstanpy:input tempfile: /tmp/tmpj7eup78t/vskayw6o.json\n",
      "DEBUG:cmdstanpy:idx 0\n",
      "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
      "DEBUG:cmdstanpy:CmdStan args: ['/home/datascience/conda/tensorflow28_p38_cpu_v1/lib/python3.8/site-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=11136', 'data', 'file=/tmp/tmpj7eup78t/i522wqp9.json', 'init=/tmp/tmpj7eup78t/vskayw6o.json', 'output', 'file=/tmp/tmpj7eup78t/prophet_modeln9pfd20o/prophet_model-20231020075554.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:55:54 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:cmdstanpy:Chain [1] start processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:55:54 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:cmdstanpy:Chain [1] done processing\n",
      "Average in input value is 2.9615368188352806\n",
      "Max input value 3.146609116439366\n",
      "Min input value 2.6079613863678617\n",
      " ------------------- \n",
      "Average in forecast value is 2.723408001576514\n",
      "Max forecast value 2.8388546167851665\n",
      "Min forecast value 2.6079613863678617\n",
      " ------------------- \n",
      "Expected increase or decreases in the coming 10 minutes is -8.04%\n",
      "Expected increase/decrease in max -9.78%\n",
      "Expected increase/decrease in min 0.0%\n",
      "\n",
      "-----------------------\n",
      "Table updated with results for EXTRACT_LAG\n",
      "-----------------------\n",
      "-------------------------------------------------------\n",
      "Start variable DATA_PUMP_READ_LAG\n",
      "<class 'str'>\n",
      "Start time of DATA_PUMP_READ_LAG is 2023-09-23 09:12:30.037776\n",
      "End time of DATA_PUMP_READ_LAG is 2023-09-23 09:42:30.010488\n",
      "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
      "INFO:prophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
      "DEBUG:cmdstanpy:input tempfile: /tmp/tmpj7eup78t/l820kx_s.json\n",
      "DEBUG:cmdstanpy:input tempfile: /tmp/tmpj7eup78t/anz8t4k9.json\n",
      "DEBUG:cmdstanpy:idx 0\n",
      "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
      "DEBUG:cmdstanpy:CmdStan args: ['/home/datascience/conda/tensorflow28_p38_cpu_v1/lib/python3.8/site-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=38158', 'data', 'file=/tmp/tmpj7eup78t/l820kx_s.json', 'init=/tmp/tmpj7eup78t/anz8t4k9.json', 'output', 'file=/tmp/tmpj7eup78t/prophet_modelir6h3l5g/prophet_model-20231020075554.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:55:54 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:cmdstanpy:Chain [1] start processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:55:54 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:cmdstanpy:Chain [1] done processing\n",
      "Average in input value is 0.5311845894439112\n",
      "Max input value 0.7441221135\n",
      "Min input value 0.31829522650402103\n",
      " ------------------- \n",
      "Average in forecast value is 0.3662016137154109\n",
      "Max forecast value 0.4141080009268008\n",
      "Min forecast value 0.31829522650402103\n",
      " ------------------- \n",
      "Expected increase or decreases in the coming 10 minutes is -31.06%\n",
      "Expected increase/decrease in max -44.35%\n",
      "Expected increase/decrease in min 0.0%\n",
      "\n",
      "-----------------------\n",
      "Table updated with results for DATA_PUMP_READ_LAG\n",
      "-----------------------\n",
      "-------------------------------------------------------\n",
      "Start variable REPLICAT_READ_LAG\n",
      "<class 'str'>\n",
      "Start time of REPLICAT_READ_LAG is 2023-09-23 09:12:30.037776\n",
      "End time of REPLICAT_READ_LAG is 2023-09-23 09:42:30.010488\n",
      "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
      "INFO:prophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
      "DEBUG:cmdstanpy:input tempfile: /tmp/tmpj7eup78t/swp2mbgn.json\n",
      "DEBUG:cmdstanpy:input tempfile: /tmp/tmpj7eup78t/jnmas77_.json\n",
      "DEBUG:cmdstanpy:idx 0\n",
      "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
      "DEBUG:cmdstanpy:CmdStan args: ['/home/datascience/conda/tensorflow28_p38_cpu_v1/lib/python3.8/site-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=86358', 'data', 'file=/tmp/tmpj7eup78t/swp2mbgn.json', 'init=/tmp/tmpj7eup78t/jnmas77_.json', 'output', 'file=/tmp/tmpj7eup78t/prophet_modelvawrzfyn/prophet_model-20231020075554.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:55:54 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:cmdstanpy:Chain [1] start processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:55:55 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:cmdstanpy:Chain [1] done processing\n",
      "Average in input value is 0.5228277573778743\n",
      "Max input value 0.6110032993219999\n",
      "Min input value 0.48040100695163634\n",
      " ------------------- \n",
      "Average in forecast value is 0.5320277838540245\n",
      "Max forecast value 0.5419439145548361\n",
      "Min forecast value 0.522111653153213\n",
      " ------------------- \n",
      "Expected increase or decreases in the coming 10 minutes is 1.76%\n",
      "Expected increase/decrease in max -11.3%\n",
      "Expected increase/decrease in min 8.68%\n",
      "\n",
      "-----------------------\n",
      "Table updated with results for REPLICAT_READ_LAG\n",
      "-----------------------\n",
      "-------------------------------------------------------\n",
      "Start variable REPLICAT_APPLY_LAG\n",
      "<class 'str'>\n",
      "Start time of REPLICAT_APPLY_LAG is 2023-09-23 09:12:30.037776\n",
      "End time of REPLICAT_APPLY_LAG is 2023-09-23 09:42:30.010488\n",
      "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
      "INFO:prophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
      "DEBUG:cmdstanpy:input tempfile: /tmp/tmpj7eup78t/sjlxcj0n.json\n",
      "DEBUG:cmdstanpy:input tempfile: /tmp/tmpj7eup78t/3ba4xwu7.json\n",
      "DEBUG:cmdstanpy:idx 0\n",
      "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
      "DEBUG:cmdstanpy:CmdStan args: ['/home/datascience/conda/tensorflow28_p38_cpu_v1/lib/python3.8/site-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=34434', 'data', 'file=/tmp/tmpj7eup78t/sjlxcj0n.json', 'init=/tmp/tmpj7eup78t/3ba4xwu7.json', 'output', 'file=/tmp/tmpj7eup78t/prophet_modelay8nfs1x/prophet_model-20231020075555.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:55:55 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:cmdstanpy:Chain [1] start processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:55:55 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:cmdstanpy:Chain [1] done processing\n",
      "Average in input value is 0.5496867328923013\n",
      "Max input value 0.6719069600978214\n",
      "Min input value 0.21251572081599998\n",
      " ------------------- \n",
      "Average in forecast value is 0.6556374746301584\n",
      "Max forecast value 0.6719069600978214\n",
      "Min forecast value 0.6393679891624955\n",
      " ------------------- \n",
      "Expected increase or decreases in the coming 10 minutes is 19.27%\n",
      "Expected increase/decrease in max 0.0%\n",
      "Expected increase/decrease in min 200.86%\n",
      "\n",
      "-----------------------\n",
      "Table updated with results for REPLICAT_APPLY_LAG\n",
      "-----------------------\n",
      "-------------------------------------------------------\n",
      "Start variable TOTAL_LAG\n",
      "<class 'str'>\n",
      "Start time of TOTAL_LAG is 2023-09-23 09:12:30.037776\n",
      "End time of TOTAL_LAG is 2023-09-23 09:42:30.010488\n",
      "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
      "INFO:prophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
      "DEBUG:cmdstanpy:input tempfile: /tmp/tmpj7eup78t/hunteetn.json\n",
      "DEBUG:cmdstanpy:input tempfile: /tmp/tmpj7eup78t/077jd9gg.json\n",
      "DEBUG:cmdstanpy:idx 0\n",
      "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
      "DEBUG:cmdstanpy:CmdStan args: ['/home/datascience/conda/tensorflow28_p38_cpu_v1/lib/python3.8/site-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=75732', 'data', 'file=/tmp/tmpj7eup78t/hunteetn.json', 'init=/tmp/tmpj7eup78t/077jd9gg.json', 'output', 'file=/tmp/tmpj7eup78t/prophet_model8rb1dh6v/prophet_model-20231020075555.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:55:55 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:cmdstanpy:Chain [1] start processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:55:56 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:cmdstanpy:Chain [1] done processing\n",
      "Average in input value is 4.54157445725518\n",
      "Max input value 4.832784914949162\n",
      "Min input value 3.991049017696123\n",
      " ------------------- \n",
      "Average in forecast value is 4.171208565264914\n",
      "Max forecast value 4.351368112833706\n",
      "Min forecast value 3.991049017696123\n",
      " ------------------- \n",
      "Expected increase or decreases in the coming 10 minutes is -8.16%\n",
      "Expected increase/decrease in max -9.96%\n",
      "Expected increase/decrease in min 0.0%\n",
      "\n",
      "-----------------------\n",
      "Table updated with results for TOTAL_LAG\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65c3906",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = {'file_name':'20231017T103618Z_20231017T104258Z.0.log.gz', 'bucket_name':'LagMetricFiles', 'namespace':'frzpemb9ufe8'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "67e99466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['score.py', 'input_files', '.ipynb_checkpoints', 'test_json_output.json', 'credentials', 'runtime.yaml']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test key</th>\n",
       "      <th>Test name</th>\n",
       "      <th>Result</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>runtime_env_path</td>\n",
       "      <td>Check that field MODEL_DEPLOYMENT.INFERENCE_ENV_PATH is set</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>runtime_env_python</td>\n",
       "      <td>Check that field MODEL_DEPLOYMENT.INFERENCE_PYTHON_VERSION is set to a value of 3.6 or higher</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>runtime_path_exist</td>\n",
       "      <td>Check that the file path in MODEL_DEPLOYMENT.INFERENCE_ENV_PATH is correct.</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>runtime_version</td>\n",
       "      <td>Check that field MODEL_ARTIFACT_VERSION is set to 3.0</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>runtime_yaml</td>\n",
       "      <td>Check that the file \"runtime.yaml\" exists and is in the top level directory of the artifact directory</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>score_load_model</td>\n",
       "      <td>Check that load_model() is defined</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>score_predict</td>\n",
       "      <td>Check that predict() is defined</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>score_predict_arg</td>\n",
       "      <td>Check that all other arguments in predict() are optional and have default values</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>score_predict_data</td>\n",
       "      <td>Check that the only required argument for predict() is named \"data\"</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>score_py</td>\n",
       "      <td>Check that the file \"score.py\" exists and is in the top level directory of the artifact directory</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>score_syntax</td>\n",
       "      <td>Check for Python syntax errors</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Test key  \\\n",
       "0     runtime_env_path   \n",
       "1   runtime_env_python   \n",
       "2   runtime_path_exist   \n",
       "3      runtime_version   \n",
       "4         runtime_yaml   \n",
       "5     score_load_model   \n",
       "6        score_predict   \n",
       "7    score_predict_arg   \n",
       "8   score_predict_data   \n",
       "9             score_py   \n",
       "10        score_syntax   \n",
       "\n",
       "                                                                                                Test name  \\\n",
       "0                                             Check that field MODEL_DEPLOYMENT.INFERENCE_ENV_PATH is set   \n",
       "1           Check that field MODEL_DEPLOYMENT.INFERENCE_PYTHON_VERSION is set to a value of 3.6 or higher   \n",
       "2                             Check that the file path in MODEL_DEPLOYMENT.INFERENCE_ENV_PATH is correct.   \n",
       "3                                                   Check that field MODEL_ARTIFACT_VERSION is set to 3.0   \n",
       "4   Check that the file \"runtime.yaml\" exists and is in the top level directory of the artifact directory   \n",
       "5                                                                      Check that load_model() is defined   \n",
       "6                                                                         Check that predict() is defined   \n",
       "7                        Check that all other arguments in predict() are optional and have default values   \n",
       "8                                     Check that the only required argument for predict() is named \"data\"   \n",
       "9       Check that the file \"score.py\" exists and is in the top level directory of the artifact directory   \n",
       "10                                                                         Check for Python syntax errors   \n",
       "\n",
       "    Result Message  \n",
       "0   Passed          \n",
       "1   Passed          \n",
       "2   Passed          \n",
       "3   Passed          \n",
       "4   Passed          \n",
       "5   Passed          \n",
       "6   Passed          \n",
       "7   Passed          \n",
       "8   Passed          \n",
       "9   Passed          \n",
       "10  Passed          "
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all should be passed\n",
    "artifact.introspect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "cdd7012b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop1:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'ocid1.datasciencemodel.oc1.eu-frankfurt-1.amaaaaaapixtsjia3z7z2spuadqer3dxtxjvagnd6rbuyn2qfwvpg5xu6hhq'"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the model artifact to the model catalog. \n",
    "import ads\n",
    "ads.set_auth(auth='resource_principal')\n",
    "\n",
    "catalog_entry = artifact.save(display_name='maersk_logs_v8', description='maersk_logs_v8', timeout=600)\n",
    "catalog_entry.id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079e08af",
   "metadata": {},
   "source": [
    "# **7. Deploy ML Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e72e69d",
   "metadata": {},
   "source": [
    "# **8. Test Deployed ML Model - Version 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "928f41b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import oci\n",
    "from oci.signer import Signer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "8639d496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "None\n",
      "CPU times: user 56.7 ms, sys: 1.41 ms, total: 58.1 ms\n",
      "Wall time: 5.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "url = \"https://modeldeployment.eu-frankfurt-1.oci.customer-oci.com/ocid1.datasciencemodeldeployment.oc1.eu-frankfurt-1.amaaaaaapixtsjiasf2ieflxolc66jkhugoklfsn5ldkvlizigc4qt65arla/predict\"\n",
    "\n",
    "#data = {'file_name':'logs.csv', 'bucket_name':'LagMetricFiles', 'namespace':'frzpemb9ufe8'}\n",
    "data = {'file_name':'20231018T004336Z_20231018T005033Z.0.log.gz', 'bucket_name':'LagMetricFiles', 'namespace':'frzpemb9ufe8'}\n",
    "\n",
    "auth = oci.auth.signers.get_resource_principals_signer()\n",
    "\n",
    "#POST request to the model\n",
    "response = requests.post(url, json=data, auth=auth)\n",
    "print(response)\n",
    "print(json.loads(response.content))\n",
    "\n",
    "full_response = json.loads(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da029bd8",
   "metadata": {},
   "source": [
    "## **Multiple requests - Version 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9324af3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example input\n",
    "import time\n",
    "df_input_extract_lag = df_input[['SOURCE_HB_TS', 'REPLICAT_APPLY_LAG']]\n",
    "df_input_extract_lag.rename(columns={\"SOURCE_HB_TS\": \"ds\", \"REPLICAT_APPLY_LAG\":\"y\"}, inplace=True)\n",
    "\n",
    "rotations = [40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]\n",
    "\n",
    "for setx in rotations:\n",
    "    #set of 30 minutes\n",
    "    input_prophet_1 = df_input_extract_lag[setx:setx+30]  #last 30 minutes, like: 40 - 70, 41 - 71, etc\n",
    "    \n",
    "    #convert to json\n",
    "    data = input_prophet_1.to_json()\n",
    "    \n",
    "    url = \"https://modeldeployment.eu-frankfurt-1.oci.customer-oci.com/ocid1.datasciencemodeldeployment.oc1.eu-frankfurt-1.amaaaaaapixtsjiaevm3vjjcyfxf2xr2k5ecrfobeo5eog5v2exoli7r4o3q/predict\"\n",
    "\n",
    "    auth = oci.auth.signers.get_resource_principals_signer()\n",
    "\n",
    "    #POST request to the model\n",
    "    response = requests.post(url, json=data, auth=auth)\n",
    "    full_response = json.loads(response.content)\n",
    "    print()\n",
    "    print(\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\")\n",
    "    print(\"Set number is \" + str(setx))\n",
    "    print(\"**Predicted increase or decrease in the coming 10 minutes = **\" +str(full_response['diff_perc_input_vs_forecast']))\n",
    "    print(\"Average number last 30 minutes = \" + str(full_response['avg_input']))\n",
    "    print(\"Average forecasted number future 10 minutes = \" + str(full_response['avg_forecast']))\n",
    "    print(\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\")\n",
    "    print()\n",
    "    time.sleep(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75454f59",
   "metadata": {},
   "source": [
    "## **Multiple requests - Version 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1976200",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example input\n",
    "import time\n",
    "df_input_extract_lag = df_input[['SOURCE_HB_TS', 'REPLICAT_APPLY_LAG']]\n",
    "df_input_extract_lag.rename(columns={\"SOURCE_HB_TS\": \"ds\", \"REPLICAT_APPLY_LAG\":\"y\"}, inplace=True)\n",
    "\n",
    "rotations = [40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]\n",
    "\n",
    "for setx in rotations:\n",
    "    #set of 30 minutes\n",
    "    input_prophet_1 = df_input_extract_lag[setx:setx+30]  #last 30 minutes, like: 40 - 70, 41 - 71, etc\n",
    "    \n",
    "    #convert to json\n",
    "    data = input_prophet_1.to_json()\n",
    "    \n",
    "    url = \"https://modeldeployment.eu-frankfurt-1.oci.customer-oci.com/ocid1.datasciencemodeldeployment.oc1.eu-frankfurt-1.amaaaaaapixtsjian3qrkhcw5wazrbquy7nbovnqnrzi2cpr45ikbseywr6a/predict\"\n",
    "\n",
    "    auth = oci.auth.signers.get_resource_principals_signer()\n",
    "\n",
    "    #POST request to the model\n",
    "    response = requests.post(url, json=data, auth=auth)\n",
    "    full_response = json.loads(response.content)\n",
    "    print()\n",
    "    print(\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\")\n",
    "    print(\"Set number is \" + str(setx))\n",
    "    print(\"**Predicted increase or decrease in the coming 10 minutes = **\" +str(full_response['diff_perc_input_vs_forecast']))\n",
    "    print(\"Average number last 30 minutes = \" + str(full_response['avg_input']))\n",
    "    print(\"Average forecasted number future 10 minutes = \" + str(full_response['avg_forecast']))\n",
    "    print(\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\")\n",
    "    print()\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb1f699",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b123781",
   "metadata": {},
   "source": [
    "# **Python script in Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "074b4393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_object(bucketName, objectName):\n",
    "#     signer = oci.auth.signers.get_resource_principals_signer()\n",
    "#     client = oci.object_storage.ObjectStorageClient(config={}, signer=signer)\n",
    "#     namespace = client.get_namespace().data\n",
    "#     try:\n",
    "#         print(\"Searching for bucket and object\", flush=True)\n",
    "#         object = client.get_object(namespace, bucketName, objectName)\n",
    "#         print(\"found object\", flush=True)\n",
    "#         if object.status == 200:\n",
    "#             print(\"Success: The object \" + objectName + \" was retrieved with the content: \" + object.data.text, flush=True)\n",
    "#             message = object.data.text\n",
    "#         else:\n",
    "#             message = \"Failed: The object \" + objectName + \" could not be retrieved.\"\n",
    "#     except Exception as e:\n",
    "#         message = \"Failed: \" + str(e.message)\n",
    "#     return { \"content\": message }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa2ef33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## example data from Events to Functions\n",
    "\n",
    "data = {\n",
    "  \"cloudEventsVersion\": \"0.1\",\n",
    "  \"eventID\": \"unique_ID\",\n",
    "  \"eventType\": \"com.oraclecloud.objectstorage.createobject\",\n",
    "  \"source\": \"objectstorage\",\n",
    "  \"eventTypeVersion\": \"2.0\",\n",
    "  \"eventTime\": \"2019-01-10T21:19:24.000Z\",\n",
    "  \"contentType\": \"application/json\",\n",
    "  \"extensions\": {\n",
    "    \"compartmentId\": \"ocid1.compartment.oc1..unique_ID\"\n",
    "  },\n",
    "  \"data\": {\n",
    "    \"compartmentId\": \"ocid1.compartment.oc1..unique_ID\",\n",
    "    \"compartmentName\": \"example_name\",\n",
    "    \"resourceName\": \"my_object\",\n",
    "    \"resourceId\": \"/n/example_namespace/b/my_bucket/o/my_object\",\n",
    "    \"availabilityDomain\": \"all\",\n",
    "    \"additionalDetails\": {\n",
    "      \"eTag\": \"f8ffb6e9-f602-460f-a6c0-00b5abfa24c7\",\n",
    "      \"namespace\": \"example_namespace\",\n",
    "      \"bucketName\": \"my_bucket\",\n",
    "      \"bucketId\": \"ocid1.bucket.oc1.phx.unique_id\",\n",
    "      \"archivalState\": \"Available\"\n",
    "    }\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3639f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['data']['resourceName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2738388a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import oci\n",
    "from datetime import datetime, timezone\n",
    "import requests\n",
    "from oci.signer import Signer\n",
    "import json\n",
    "\n",
    "auth = oci.auth.signers.get_resource_principals_signer()\n",
    "object_storage_client = oci.object_storage.ObjectStorageClient(config={}, signer=signer)\n",
    "\n",
    "def handler(ctx, data: io.BytesIO = None):\n",
    "    \n",
    "    bucket_name = \"LagMetricFiles\"\n",
    "    namespace = \"frzpemb9ufe8\"\n",
    "    #sub_bucket = \"LagMetrics/ocid1.serviceconnector.oc1.eu-frankfurt-1.amaaaaaapixtsjiarpunoxhi6tvcw3jdbgwyt6xzv4cl4zryhuigxesyyjmq\"\n",
    "    \n",
    "    latest_file_name = data['data']['resourceName']\n",
    "\n",
    "    #cal the HTTP endpoint\n",
    "    data = {'file_name':latest_file_name, 'bucket_name':bucket_name, 'namespace':namespace}\n",
    "\n",
    "    url = \"https://modeldeployment.eu-frankfurt-1.oci.customer-oci.com/ocid1.datasciencemodeldeployment.oc1.eu-frankfurt-1.amaaaaaapixtsjia7wjigtlgiqvy652ymano5nrfcpsf7piptjx77s2xylia/predict\"\n",
    "\n",
    "    response = requests.post(url, json=data, auth=auth)\n",
    "    full_response = json.loads(response.content)\n",
    "    print(full_response) #for logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67c807e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'file_name':'20231017T103618Z_20231017T104258Z.0.log.gz', 'bucket_name':'LagMetricFiles', 'namespace':'frzpemb9ufe8'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95781d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow28_p38_cpu_v1]",
   "language": "python",
   "name": "conda-env-tensorflow28_p38_cpu_v1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
